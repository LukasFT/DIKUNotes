<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="max-flow">Max Flow</h1>
<h2 id="problem-definition">Problem definition</h2>
<p>Find maximum flow in directed graph from source <span class="math inline"><em>s</em></span> to sink <span class="math inline"><em>t</em></span>. More formally, we have:</p>
<dl>
<dt>Flow network graph:</dt>
<dd><p>Directed graph <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span>, without antiparallel edges or self-loops and with nodes <span class="math inline"><em>s</em></span> (source) and <span class="math inline"><em>t</em></span> (sink).</p>
</dd>
<dt>Capacity function:</dt>
<dd><p><span class="math inline">$c(u, v) = \begin{cases} &gt; 0 &amp; \text{if $</span>(u, v) E<span class="math inline">$} \\ 0 &amp; \text{otherwise}\end{cases}$</span>.</p>
</dd>
<dt>Flows:</dt>
<dd><p><span class="math inline"><em>f</em> : <em>V</em> × <em>V</em> → ℝ</span> which satisfies the constraints:</p>
</dd>
<dt>Capacity constraint:</dt>
<dd><p><span class="math inline">∀<em>u</em>, <em>v</em> ∈ <em>V</em> 0 ≤ <em>f</em>(<em>u</em>, <em>v</em>)≤<em>c</em>(<em>u</em>, <em>v</em>)</span></p>
</dd>
<dt>Flow conservation:</dt>
<dd><p><span class="math inline">∀<em>u</em> ∈ <em>V</em> − <em>s</em>, <em>t</em> ∑<sub><em>v</em> ∈ <em>V</em></sub><em>f</em>(<em>v</em>, <em>u</em>)=∑<sub><em>v</em> ∈ <em>V</em></sub><em>f</em>(<em>u</em>, <em>v</em>)</span></p>
</dd>
<dt>Flow value:</dt>
<dd><p>Find flow of maximum <em>value</em>: <span class="math inline">|<em>f</em>|=∑<sub><em>v</em> ∈ <em>V</em></sub><em>f</em>(<em>s</em>, <em>v</em>)−∑<sub><em>v</em> ∈ <em>V</em></sub><em>f</em>(<em>v</em>, <em>s</em>)</span></p>
</dd>
</dl>
<h2 id="ford-fulkerson-method">Ford-Fulkerson method</h2>
<p>The Ford-Fulkerson method uses residual networks to find “augmenting paths” — paths along which can be sent more flow.</p>
<p>Initialize flow <span class="math inline"><em>f</em></span> to 0 augment flow <span class="math inline"><em>f</em></span> along <span class="math inline"><em>p</em></span> <span class="math inline"><em>f</em></span></p>
<h3 id="residual-networks">Residual Networks</h3>
<p>A residual network is a graph <span class="math inline"><em>G</em><sub><em>f</em></sub></span> which represents how we can change the flow <span class="math inline"><em>f</em></span> on the network graph <span class="math inline"><em>G</em></span>.</p>
<p>Given flow network <span class="math inline"><em>G</em> = (<em>V</em>, <em>E</em>)</span> and flow <span class="math inline"><em>f</em></span>, we have the <strong>residual capacity</strong> <span class="math inline"><em>c</em><sub><em>f</em></sub>(<em>u</em>, <em>v</em>)</span>: <br /><span class="math display">$$c_f(u, v) =
\begin{cases}
    c(u, v) - f(u, v) &amp; (u, v) \in E \\
    f(v, u)           &amp; (v, u) \in E \\
    0                 &amp; \text{otherwise}
\end{cases}$$</span><br /></p>
<h3 id="augmentation-of-flow-f-by-f">Augmentation of flow <span class="math inline"><em>f</em></span> by <span class="math inline"><em>f</em>′</span></h3>
<p><br /><span class="math display">$$(f \uparrow f')(u, v) =
\begin{cases}
    f(u, v) + f'(u, v) - f'(v, u) &amp; (u, v) \in E \\
    0                             &amp; \text{otherwise}
\end{cases}$$</span><br /> Important thing is that <span class="math inline">|<em>f</em> ↑ <em>f</em>′| = |<em>f</em>|+|<em>f</em>′|</span>.</p>
<p>Augmenting path has flow: <span class="math inline">$f_p(u, v) = \begin{cases} c_f(p) &amp; (u, v) \in p \\ 0 &amp; \text{otherwise} \end{cases}$</span></p>
<p>Additionally: <span class="math inline">|<em>f</em><sub><em>p</em></sub>|=<em>c</em><sub><em>f</em></sub>(<em>p</em>)&gt;0</span></p>
<p>This implies: <span class="math inline">|<em>f</em> ↑ <em>f</em><sub><em>p</em></sub>|=|<em>f</em>|+|<em>f</em><sub><em>p</em></sub>|&gt;|<em>f</em>|</span></p>
<p>Example of network with residual network:</p>
<p><img src="flowexample.png" alt="image" /> [flowexample]</p>
<h2 id="cuts">Cuts</h2>
<p>Partitions vertices into two groups with one containing <span class="math inline"><em>s</em></span>, the other containing <span class="math inline"><em>t</em></span>.</p>
<p>Net flow of cut: <br /><span class="math display"><em>f</em>(<em>S</em>, <em>T</em>)=∑<sub><em>u</em> ∈ <em>S</em></sub>∑<sub><em>v</em> ∈ <em>T</em></sub><em>f</em>(<em>u</em>, <em>v</em>)−∑<sub><em>u</em> ∈ <em>S</em></sub>∑<sub><em>v</em> ∈ <em>T</em></sub><em>f</em>(<em>v</em>, <em>u</em>)=|<em>f</em>|</span><br /> Capacity of cut: <br /><span class="math display"><em>c</em>(<em>S</em>, <em>T</em>)=∑<sub><em>u</em> ∈ <em>S</em></sub>∑<sub><em>v</em> ∈ <em>T</em></sub><em>c</em>(<em>u</em>, <em>v</em>)≥|<em>f</em>|</span><br /></p>
<h2 id="max-flow-min-cut-theorem">Max-flow min-cut theorem</h2>
<p>The following statements are equivalent:</p>
<ol>
<li><p><span class="math inline"><em>f</em></span> is a maximum flow in <span class="math inline"><em>G</em></span>.</p></li>
<li><p>The residual network <span class="math inline"><em>G</em><sub><em>f</em></sub></span> contains no augmenting paths.</p></li>
<li><p><span class="math inline">|<em>f</em>|=<em>c</em>(<em>S</em>, <em>T</em>)</span> for some cut <span class="math inline">(<em>S</em>, <em>T</em>)</span> of <span class="math inline"><em>G</em></span>.</p></li>
</ol>
<dl>
<dt><span class="math inline">1 ⇒ 2</span>:</dt>
<dd><p>If augmenting path exists, <span class="math inline">|<em>f</em>|</span> can be increased by augmentation.</p>
</dd>
<dt><span class="math inline">2 ⇒ 3</span>:</dt>
<dd><p>Cut <span class="math inline">(<em>S</em>, <em>T</em>)</span> where <span class="math inline"><em>S</em></span> is all vertices reachable from <span class="math inline"><em>s</em></span> and <span class="math inline"><em>T</em></span> are the others. Then:<br />
<span class="math inline">∀<em>u</em> ∈ <em>S</em>, <em>v</em> ∈ <em>T</em> (<em>u</em>, <em>v</em>)∈<em>E</em> ⇒ <em>f</em>(<em>u</em>, <em>v</em>)=<em>c</em>(<em>u</em>, <em>v</em>)</span> since otherwise <span class="math inline">(<em>u</em>, <em>v</em>)∈<em>E</em><sub><em>f</em></sub></span><br />
<span class="math inline">∀<em>u</em> ∈ <em>S</em>, <em>v</em> ∈ <em>T</em> (<em>v</em>, <em>u</em>)∈<em>E</em> ⇒ <em>f</em>(<em>v</em>, <em>u</em>)=0</span> since otherwise <span class="math inline">(<em>u</em>, <em>v</em>)∈<em>E</em><sub><em>f</em></sub></span><br />
<span class="math inline">∀<em>u</em> ∈ <em>S</em>, <em>v</em> ∈ <em>T</em> (<em>u</em>, <em>v</em>)∉<em>E</em> ∧ (<em>v</em>, <em>u</em>)∉<em>E</em> ⇒ <em>f</em>(<em>u</em>, <em>v</em>)=<em>f</em>(<em>v</em>, <em>u</em>)=0</span> Hence: <br /><span class="math display">$$\begin{aligned}
        f(S , T) &amp;= \sum_{u \in S} \sum_{v \in T} f(u , v) - \sum_{v \in T} \sum_{u \in S} f(v , u) \\
                 &amp;= \sum_{u \in S} \sum_{v \in T} c(u , v) - \sum_{v \in T} \sum_{u \in S} 0 \\
                 &amp;= c(S , T) = |f| &amp; \text{by Lemma 26.4}
    \end{aligned}$$</span><br /></p>
</dd>
<dt><span class="math inline">3 ⇒ 1</span>:</dt>
<dd><p><span class="math inline">|<em>f</em>|≤<em>c</em>(<em>S</em>, <em>T</em>)</span>, therefore <span class="math inline">|<em>f</em>|=<em>c</em>(<em>S</em>, <em>T</em>)</span> must be maximal.</p>
</dd>
</dl>
<h2 id="edmonds-karp-algorithm">Edmonds-Karp algorithm</h2>
<p>Uses shortest unit-distance (breadth-first search) to decide augmenting path.</p>
<h3 id="running-time">Running time</h3>
<p>Each iteration takes <span class="math inline"><em>O</em>(<em>V</em> + <em>E</em>)=<em>O</em>(<em>E</em>)</span> time, so we need to simply limit the number of iterations.</p>
<p>Forward edge in residual network: <span class="math inline">(<em>u</em>, <em>v</em>)∈<em>E</em><sub><em>f</em></sub></span> such that <span class="math inline"><em>δ</em><sub><em>f</em></sub>(<em>s</em>, <em>v</em>)=<em>δ</em><sub><em>f</em></sub>(<em>s</em>, <em>u</em>)+1</span>.</p>
<p><strong>Lemma:</strong> <span class="math inline"><em>G</em><sub>0</sub></span> is residual graph at some point. <span class="math inline"><em>f</em><sub>1</sub>, …, <em>f</em><sub><em>k</em></sub></span> is some sequence of flows obtained during the next <span class="math inline"><em>k</em></span> iterations. Let <span class="math inline"><em>d</em> = <em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>t</em>)</span>. Then, if <span class="math inline"><em>δ</em><sub><em>f</em><sub><em>i</em></sub></sub>(<em>s</em>, <em>t</em>)=<em>d</em></span> for <span class="math inline"><em>i</em> = 1, …, <em>k</em> − 1</span>, then flow is only pushed along forward edges in <span class="math inline"><em>G</em><sub><em>f</em><sub>0</sub></sub></span> and <span class="math inline"><em>δ</em><sub><em>f</em><sub><em>k</em></sub></sub>(<em>s</em>, <em>t</em>)≥<em>d</em></span>.</p>
<p><strong>Proof by induction on k:</strong> Base: <span class="math inline"><em>k</em> = 0</span>, true since no iterations are done, <span class="math inline"><em>G</em><sub><em>f</em><sub>0</sub></sub> = <em>G</em><sub><em>f</em><sub><em>k</em></sub></sub></span>.</p>
<p>Step: Assume <span class="math inline"><em>k</em> &gt; 0</span> and it holds for <span class="math inline"><em>k</em> − 1</span>. The algorithm will push flow along the shortest path <span class="math inline"><em>p</em></span> in <span class="math inline"><em>G</em><sub><em>f</em><sub><em>k</em> − 1</sub></sub></span>.</p>
<p>For every edge <span class="math inline">(<em>u</em>, <em>v</em>)</span> in a shortest path <span class="math inline"><em>p</em></span> in <span class="math inline"><em>G</em><sub><em>f</em><sub><em>k</em> − 1</sub></sub></span> we must have <span class="math inline"><em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>v</em>)≤<em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>u</em>)+1</span>, since flow has only been pushed along forward edges.</p>
<p>But since length of <span class="math inline"><em>p</em></span> is <span class="math inline"><em>d</em></span>, we must have <span class="math inline"><em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>v</em>)=<em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>u</em>)+1</span> for every edge <span class="math inline">(<em>u</em>, <em>v</em>)∈<em>p</em></span>.</p>
<p>Similarly, for any edge <span class="math inline">(<em>u</em>, <em>v</em>)</span> in any shortest path <span class="math inline"><em>p</em></span> from <span class="math inline"><em>s</em></span> to <span class="math inline"><em>t</em></span> in <span class="math inline"><em>G</em><sub><em>f</em><sub><em>k</em></sub></sub></span> we must have <span class="math inline"><em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>v</em>)≤<em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>u</em>)+1</span>. Since <span class="math inline"><em>p</em></span> must have at least <span class="math inline"><em>d</em></span> edges, <span class="math inline"><em>δ</em><sub><em>f</em><sub><em>k</em></sub></sub>(<em>s</em>, <em>t</em>)≥<em>d</em></span>. QED.</p>
<p><strong>Theorem:</strong> Edmonds-Karp runs in <span class="math inline"><em>O</em>(<em>V</em><em>E</em><sup>2</sup>)</span>.</p>
<p><strong>Proof:</strong> Each iteration takes <span class="math inline"><em>O</em>(<em>V</em> + <em>E</em>)=<em>O</em>(<em>E</em>)</span> time. Must show number of iterations is <span class="math inline"><em>O</em>(<em>V</em><em>E</em>)</span>.</p>
<p>Maximum-length sequence of consecutive flows <span class="math inline"><em>f</em><sub>0</sub>, …, <em>f</em><sub><em>k</em></sub></span> such that <span class="math inline"><em>δ</em><sub><em>f</em><sub><em>i</em></sub></sub> = <em>d</em></span> for <span class="math inline"><em>i</em> = 1, …, <em>k</em> − 1</span>, where <span class="math inline"><em>d</em> = <em>δ</em><sub><em>f</em><sub>0</sub></sub>(<em>s</em>, <em>t</em>)</span>. By Lemma 1 <span class="math inline"><em>k</em> ≤ |<em>E</em>|</span> since each iteration removes an edge. Must also have <span class="math inline"><em>δ</em><sub><em>f</em><sub><em>k</em></sub></sub>(<em>s</em>, <em>t</em>)≥<em>d</em> + 1</span> by maximality of the sequence.</p>
<p>Hence, <span class="math inline"><em>δ</em>(<em>s</em>, <em>t</em>)</span> must increase by 1 at least every <span class="math inline">|<em>E</em>|</span> iterations. The distance can’t be more than <span class="math inline">|<em>V</em>|−1</span>. Therefore, number of iterations <span class="math inline"><em>O</em>(<em>V</em><em>E</em>)</span>.</p>
<h2 id="other-stuff">Other stuff</h2>
<p><br /><span class="math display">$$\begin{aligned}
 \left( f \uparrow f ^ { \prime } \right) ( u , v ) &amp; = f ( u , v ) + f ^ { \prime } ( u , v ) - f ^ { \prime } ( v , u ) \\ &amp; \geq f ( u , v ) + f ^ { \prime } ( u , v ) - f ( u , v ) \\ &amp; = f ^ { \prime } ( u , v ) \\ &amp; \geq 0 \end{aligned}$$</span><br /></p>
<p><br /><span class="math display">$$\begin{aligned}
 \left( f \uparrow f ^ { \prime } \right) ( u , v ) &amp; = f ( u , v ) + f ^ { \prime } ( u , v ) - f ^ { \prime } ( v , u ) \\ &amp; \leq f ( u , v ) + f ^ { \prime } ( u , v ) \\ &amp; \leq f ( u , v ) + c _ { f } ( u , v ) \\ &amp; = f ( u , v ) + c ( u , v ) - f ( u , v ) \\ &amp; = c ( u , v ) \end{aligned}$$</span><br /></p>
<p><img src="flowcon.png" alt="image" /></p>
<h1 id="linear-programming">Linear Programming</h1>
<h2 id="motivation">Motivation</h2>
<p>Often you have optimization problems under constraints. You’d like a general algorithm to solve such problems and find the optimal solution that fits all of the constraints.</p>
<h2 id="linear-program-forms">Linear program forms</h2>
<p>Standard form: <br /><span class="math display">$$\begin{array}{lrclll}
    \text{maximize}   &amp; \displaystyle\sum_{j = 1}^n c_j x_j    &amp;      &amp;      &amp;                                &amp; \text{or } c^\top x \\
    \text{subject to} &amp;                                        &amp;      &amp;      &amp;                                &amp; \\
                      &amp; \displaystyle\sum_{j = 1}^n a_{ij} x_j &amp; \leq &amp; b_i  &amp; \text{for } i = 1, 2, \dots, m &amp; \text{or } Ax \leq b \\
                      &amp; x_j                                    &amp; \geq &amp; 0    &amp; \text{for } j = 1, 2, \dots, n &amp; \text{or } x \geq 0
\end{array}$$</span><br /> A linear program: <br /><span class="math display">$$\begin{array}{lrclll}
    \text{minimize}   &amp; -2x_1    &amp; + &amp; 3x_2 &amp;      &amp; \\
    \text{subject to} &amp;          &amp;   &amp;      &amp;      &amp; \\
                      &amp; x_1      &amp; + &amp; x_2  &amp; =    &amp; 7 \\
                      &amp; x_1      &amp; - &amp; 2x_2 &amp; \leq &amp; 4 \\
                      &amp; x_1      &amp;   &amp;      &amp; \geq &amp; 0
\end{array}$$</span><br /> Same linear program in standard form: <br /><span class="math display">$$\begin{array}{lrclllll}
    \text{maximize}   &amp; 2x_1     &amp; - &amp; 3x_2 &amp; + &amp; 3x_3 &amp;      &amp;    \\
    \text{subject to} &amp;          &amp;   &amp;      &amp;   &amp;      &amp;      &amp;    \\
                      &amp; x_1      &amp; + &amp; x_2  &amp; - &amp; x_3  &amp; \leq &amp; 7  \\
                      &amp; -x_1     &amp; - &amp; x_2  &amp; + &amp; x_3  &amp; \leq &amp; -7 \\
                      &amp; x_1      &amp; - &amp; 2x_2 &amp; + &amp; 2x_3 &amp; \leq &amp; 4  \\
                      &amp; x_1, x_2, x_3 &amp; &amp;   &amp;   &amp;      &amp; \geq &amp; 0
\end{array}$$</span><br /> Same linear program in slack form: <br /><span class="math display">$$\begin{array}{rrrrrrrrr}
z   &amp; = &amp;    &amp;   &amp; 2x_1 &amp; - &amp; 3x_2 &amp; + &amp; 3x_3 \\
x_4 &amp; = &amp; 7  &amp; - &amp; x_1  &amp; - &amp; x_2  &amp; + &amp; x_3  \\
x_5 &amp; = &amp; -7 &amp; + &amp; x_1  &amp; + &amp; x_2  &amp; - &amp; x_3  \\
x_6 &amp; = &amp; 4  &amp; - &amp; x_1  &amp; + &amp; 2x_2 &amp; - &amp; 2x_3
\end{array}$$</span><br /></p>
<h2 id="simplex">Simplex</h2>
<p>Algorithm is essentially just this:</p>
<ol>
<li><p>Find non-basic variable <span class="math inline"><em>x</em></span> with positive coefficient</p></li>
<li><p>Check how much you can increase <span class="math inline"><em>x</em></span> without violating non-negative constraints</p></li>
<li><p>Pick the constraint with the lowest (limiting) amount</p></li>
<li><p>Rewrite constraint to make <span class="math inline"><em>x</em></span> a basic variable (isolate <span class="math inline"><em>x</em></span> in the constraint)</p></li>
<li><p>Insert the new equation for <span class="math inline"><em>x</em></span> into <span class="math inline"><em>x</em></span>’s place whereever (pivoting)</p></li>
<li><p><span class="math inline"><em>z</em></span> now has a higher value with non-basic variables set to 0</p></li>
<li><p>Repeat until no non-basic variables with positive coefficients</p></li>
</ol>
<p>Unfortunately might take exponential time on contrived input. Usually very effective though.</p>
<h2 id="duality">Duality</h2>
<p>To prove optimality, a concept of duality is used.</p>
<p>Switch coefficients and targets, switch <span class="math inline">≤</span> to <span class="math inline">≥</span>, switch <span class="math inline"><em>x</em><sub><em>j</em></sub></span> for <span class="math inline"><em>y</em><sub><em>i</em></sub></span>, switch <span class="math inline"><em>n</em></span> for <span class="math inline"><em>m</em></span>. Then:</p>
<p><br /><span class="math display">$$\begin{array}{lrclll}
    \text{minimize}   &amp; \displaystyle\sum_{i = 1}^m b_i y_i    &amp;      &amp;      &amp;                                &amp; \\
    \text{subject to} &amp;                                        &amp;      &amp;      &amp;                                &amp; \\
                      &amp; \displaystyle\sum_{i = 1}^m a_{ij} y_i &amp; \geq &amp; c_j  &amp; \text{for } j = 1, 2, \dots, n &amp; \\
                      &amp; y_i                                    &amp; \geq &amp; 0    &amp; \text{for } i = 1, 2, \dots, m &amp;
\end{array}$$</span><br /></p>
<p>Then prove: <br /><span class="math display">$$\begin{aligned}
    \sum_{j = 1}^n c_j \bar{x}_j &amp;\leq \sum_{j = 1}^n {\left( \sum_{i = 1}^m a_{ij} \bar{y}_i \right)} \bar{x}_j \\
                                 &amp;=    \sum_{i = 1}^m {\left( \sum_{j = 1}^n a_{ij} \bar{x}_j \right)} \bar{y}_i \\
                                 &amp;\leq \sum_{i = 1}^m b_i \bar{y}_i\end{aligned}$$</span><br /></p>
<h2 id="formulating-programs-as-linear-programs">Formulating programs as linear programs</h2>
<h3 id="shortest-paths">Shortest paths</h3>
<p><br /><span class="math display">$$\begin{array}{lrcll}
    \text{maximize}   &amp; d_t      &amp;      &amp;               &amp; \\
    \text{subject to} &amp;          &amp;      &amp;               &amp; \\
                      &amp; d_v      &amp; \leq &amp; d_u + w(u, v) &amp; \text{for each edge } (u, v) \in E \\
                      &amp; d_s      &amp; =    &amp; 0             &amp;
\end{array}$$</span><br /></p>
<h3 id="maximum-flow">Maximum flow</h3>
<p><br /><span class="math display">$$\begin{array}{lrcll}
    \text{maximize} &amp; \displaystyle\sum_{v \in V} f_{sv} &amp; -    &amp; \displaystyle\sum_{v \in V} f_{vs} \\
    \text{subject to} \\
    &amp; f_{uv}                             &amp; \leq &amp; c(u, v)                            &amp; \text{for each } u, v \in V \\
    &amp; \displaystyle\sum_{v \in V} f_{uv} &amp; =    &amp; \displaystyle\sum_{v \in V} f_{vu} &amp; \text{for each } u \in V - {\left\{ s, t \right\}} \\
    &amp; f_{uv}                             &amp; \geq &amp; 0                                  &amp; \text{for each } u, v \in V
\end{array}$$</span><br /></p>
<h1 id="randomized-algorithms">Randomized Algorithms</h1>
<p>Randomized algorithms are algorithms that have some randomized component to them. Using randomization, it’s possible for some algorithms to exhibit very favourable running time and/or correctness in the <em>expected</em> case. There are two main types of randomized algorithms:</p>
<dl>
<dt>Las Vegas:</dt>
<dd><p>Always returns correct result, may take a long time.</p>
</dd>
<dt>Monte Carlo:</dt>
<dd><p>Always runs fast, may not be correct.</p>
</dd>
</dl>
<h2 id="example-las-vegas-randqs">Example Las Vegas: RandQS</h2>
<p>Random quicksort works by choosing pivots randomly. We want to get an expected running time of this algorithm.</p>
<p>Let <span class="math inline"><em>S</em></span> be the array in sorted order. Let <span class="math inline"><em>S</em><sub><em>i</em></sub></span> be the <span class="math inline"><em>i</em></span>th element of <span class="math inline"><em>S</em></span>, i.e. the <span class="math inline"><em>i</em></span>th smallest element. We count the number of comparisons the algorithm performs in order to give its bound. Let <span class="math inline"><em>X</em><sub><em>i</em><em>j</em></sub></span> be an indicator variable that is 1 if <span class="math inline"><em>S</em><sub><em>i</em></sub></span> and <span class="math inline"><em>S</em><sub><em>j</em></sub></span> are compared in the algorithm. Number of comparisons are then: <br /><span class="math display">$$\sum_{i = 1}^n \sum_{j &gt; i}^n X_{ij}$$</span><br /> We want the expected value of comparisons: <br /><span class="math display">$$\begin{aligned}
    {\mathbb{E}{{\left[ \sum_{i = 1}^n \sum_{j &gt; i}^n X_{ij} \right]}}} &amp;= \sum_{i = 1}^n \sum_{j &gt; i}^n {\mathbb{E}{{\left[ X_{ij} \right]}}} \\
    &amp;= \sum_{i = 1}^n \sum_{j &gt; i}^n \frac{2}{j - i + 1}\end{aligned}$$</span><br /> Last line follows because they are only compared if they are chosen among <span class="math inline"><em>S</em><sub><em>i</em></sub>, …, <em>S</em><sub><em>j</em></sub></span> sequence. <br /><span class="math display">$$\begin{aligned}
    \sum_{i = 1}^n \sum_{j &gt; i}^n \frac{2}{j - i + 1} &amp;\leq \sum_{i = 1}^n \sum_{k = 1}^{n - i + 1} \frac{2}{k} \\
    &amp;\leq 2 \sum_{i = 1}^n \sum_{k = 1}^{n} \frac{1}{k} \\
    &amp;= 2nH(n) \\
    &amp;= O(n \ln n)\end{aligned}$$</span><br /> Thus expected run-time (number of comparisons) is optimal.</p>
<h2 id="example-monte-carlo-randomized-min-cut">Example Monte Carlo: Randomized Min-Cut</h2>
<p>Randomly choose an edge to contract. Firstly, min-cut is never reduced by a contraction. Secondly, we may not get any optimal min-cut, since if an edge from a min-cut is chosen, it will be eliminated.</p>
<p>Probability of not choosing any edge in a particular min-cut <span class="math inline"><em>C</em></span> of size <span class="math inline"><em>k</em></span> at step <span class="math inline"><em>i</em></span> is <span class="math inline">Pr[ℰ<sub><em>i</em></sub>]</span>. There are at least <span class="math inline"><em>k</em><em>n</em>/2</span> edges because otherwise there is some vertex with less than <span class="math inline"><em>k</em></span> edges, which is then a min-cut. At step <span class="math inline"><em>i</em></span> there are at least <span class="math inline"><em>k</em>(<em>n</em> − <em>i</em> + 1)/2</span> edges left. Chance of not picking one of the <span class="math inline"><em>k</em></span> edges of <span class="math inline"><em>C</em></span> is then: <br /><span class="math display">$$\mathcal{E}_1 = 1 - \frac{k}{nk/2} = 1 - \frac{2}{n}$$</span><br /> Generally for the next iterations: <br /><span class="math display">$$\mathcal{E}_i = 1 - \frac{k}{k(n - i + 1)/2} = 1 - \frac{2}{n - i + 1}$$</span><br /> Chance of <em>not</em> picking one of these edges at any step is then: <br /><span class="math display">$$\Pr{{\left[ \bigcap_{i = 1}^{n - 2} \mathcal{E}_i \right]}} \geq \prod_{i = 1}^{n - 2} {\left( 1 - \frac{2}{n - i + 1} \right)} = \frac{2}{n {\left( n - 1 \right)}} \geq \frac{2}{n^2}$$</span><br /> We repeat the algorithm <span class="math inline"><em>n</em><sup>2</sup>/2</span> times. The chance of not finding any min-cut in any of the runs is then: <br /><span class="math display">$${\left( 1 - \frac{2}{n^2} \right)}^{n^2/2} &lt; \frac{1}{e}$$</span><br /> Further runs decrease this chance even more at the expense of more running time.</p>
<p>For decision problems, there are two kinds of Monte Carlo: One-sided error and two-sided error. One-sided only has error on one case, two-sided has error on both.</p>
<h1 id="hashing">Hashing</h1>
<h2 id="motivation-1">Motivation</h2>
<p>We have a large universe <span class="math inline"><em>U</em></span> that we wish to map randomly to a smaller range <span class="math inline">[<em>m</em>]={0,…,<em>m</em>−1}</span>. To do this perfectly would require enormous amounts of memory, so instead, we store a bit of random information which helps us to do it almost perfectly.</p>
<h2 id="hash-functions">Hash functions</h2>
<p>To do this mapping, we use hash functions: <br /><span class="math display"><em>h</em>: <em>U</em> → [<em>m</em>]</span><br /> Where <span class="math inline"><em>h</em></span> is a random variable in the class of all functions from <span class="math inline"><em>U</em></span> to <span class="math inline">[<em>m</em>]</span>.</p>
<p>For a hash to be useful it should give each value a seemingly random hash — and in doing so, preferably it should have a low <em>collision chance</em>. That is, for two keys <span class="math inline"><em>x</em>, <em>y</em></span>, very rarely should <span class="math inline"><em>h</em>(<em>x</em>)=<em>h</em>(<em>y</em>)</span>.</p>
<h2 id="universality">Universality</h2>
<p>To formalise the idea of rare collision chance, we talk about <em>universal</em> hash functions. These are hash functions where for keys <span class="math inline"><em>x</em>, <em>y</em></span> chosen independently at random, we have: <br /><span class="math display">$$\Pr {\left[ h{{\left( x \right)}} = h{{\left( y \right)}} \right]} \leq \frac{1}{m}$$</span><br /> Sometimes we only get close to this property, in which case we may have <em>c-universality</em>: <br /><span class="math display">$$\Pr {\left[ h{{\left( x \right)}} = h{{\left( y \right)}} \right]} \leq \frac{c}{m}$$</span><br /></p>
<h3 id="strong-universality">Strong Universality</h3>
<p>We call a hash function <em>strongly universal</em> if, for two distinct keys <span class="math inline"><em>x</em>, <em>y</em></span> and for two hashes <span class="math inline"><em>q</em>, <em>r</em></span>, we have: <br /><span class="math display">$$\Pr{{\left[ h(x) = q \land h(y) = r \right]}} = \frac 1{m^2}$$</span><br /> Strong universality can be generalized to <em>k-independence</em>, where strong universality is simply 2-independence.</p>
<p>Also universal because: <br /><span class="math display">Pr[<em>h</em>(<em>x</em>)=<em>h</em>(<em>y</em>)] = ∑<sub><em>q</em> ∈ [<em>m</em>]</sub>Pr[<em>h</em>(<em>x</em>)=<em>q</em> ∧ <em>h</em>(<em>y</em>)=<em>q</em>]=<em>m</em>/<em>m</em><sup>2</sup> = 1/<em>m</em></span><br /></p>
<h2 id="application-hash-table">Application: Hash table</h2>
<p>We have <span class="math inline"><em>S</em> ⊆ <em>U</em></span> that we wish to store and retrieve single key of in expected constant time. We have <span class="math inline">|<em>S</em>|=<em>n</em></span> and <span class="math inline"><em>n</em> ≤ <em>m</em></span>. Then pick hash function <span class="math inline"><em>h</em>: <em>U</em> → [<em>m</em>]</span> and create array <span class="math inline"><em>L</em></span> containing <span class="math inline"><em>m</em></span> lists. Then we can find keys in <span class="math inline"><em>S</em></span> by looking in <span class="math inline"><em>L</em>[<em>h</em>(<em>x</em>)]</span>. Now want to show that <span class="math inline"><em>L</em>[<em>h</em>(<em>x</em>)]</span> has expected size <span class="math inline">1</span>.</p>
<p>Assume that <span class="math inline"><em>x</em> ∉ <em>S</em></span> for worst case. Assume <span class="math inline"><em>h</em></span> is universal. <span class="math inline"><em>I</em>(<em>y</em>)</span> is 1 when <span class="math inline"><em>h</em>(<em>x</em>)=<em>h</em>(<em>y</em>)</span>. Then the expected length of <span class="math inline"><em>L</em>[<em>h</em>(<em>x</em>)]</span> is: <br /><span class="math display">$$\mathbb{E}{{\left[  | L{{\left[ h{{\left( x \right)}} \right]}} |  \right]}} = \mathbb{E}{{\left[  \sum_{y \in S} I(y)  \right]}} = \sum_{y \in S} \mathbb{E}{{\left[ I(y) \right]}} = \sum_{y \in S} \frac{1}{m} = \frac nm \leq 1$$</span><br /></p>
<h2 id="application-coordinated-sampling">Application: Coordinated Sampling</h2>
<p>Used for sampling randomly from too big databases. Able to estimate things about the original from the sample. For example, <span class="math inline">$\bar{|A|} = |S_{h, t}(A)| m/t$</span>. This is because:</p>
<p><br /><span class="math display"><em>S</em><sub><em>h</em>, <em>t</em></sub>(<em>A</em>)={<em>x</em>∈<em>A</em>|<em>h</em>(<em>x</em>)&lt;<em>t</em>}</span><br /></p>
<p><br /><span class="math display">$$\mathbb{E}{{\left[ |S_{h, t}| \right]}} = |A| \frac{t}{m}$$</span><br /></p>
<p>We also have the following:</p>
<p><br /><span class="math display">$$\Pr{{\left[ |X - \mu| \geq q \sqrt{\mu} \right]}} \leq \frac 1{q^2}$$</span><br /></p>
<h1 id="van-emde-boas-trees">van Emde Boas Trees</h1>
<h2 id="motivation-2">Motivation</h2>
<p>The lower bound of sorting makes it so priority queues must have an <span class="math inline"><em>O</em>(lg<em>n</em>)</span> operation. However, we can sort in linear time given assumptions on the input, so maybe we can also have faster priority queues given the same assumptions.</p>
<p>We would like a data structure capable of providing <span class="math inline"><em>O</em>(lglg<em>u</em>)</span> running times for member, insertion, deletion and finding successor and predessesor, where <span class="math inline"><em>u</em></span> is the size of the universe, that is, the values we can store.</p>
<p><strong>Binary tree:</strong> Binary tree imposed onto an array. This allows <span class="math inline"><em>O</em>(lg<em>u</em>)</span>.<br />
<strong>Square root tree:</strong> Tree of constant height by summary array of length <span class="math inline">$\sqrt{u}$</span>. Running time <span class="math inline">$O(\sqrt{u})$</span> (worse!). Also puts restrictions on the size of <span class="math inline"><em>u</em> = 2<sup>2<em>k</em></sup></span>.</p>
<h2 id="the-van-emde-boas-tree">The van Emde Boas Tree</h2>
<p>A van Emde Boas tree consists of some stored information and several clusters which are themselves van Emde Boas trees. Each tree has a universe size <span class="math inline"><em>u</em></span>. <span class="math inline"><em>u</em> = 2<sup><em>k</em></sup></span> is always a power of two. Denote helper functions: <br /><span class="math display">$$\begin{aligned}
    \sqrt[\uparrow]{u}   &amp;= 2^{\lceil (\lg u) / 2 \rceil} \\
    \sqrt[\downarrow]{u} &amp;= 2^{\lfloor (\lg u) / 2 \rfloor} \\
    \text{high}(x)       &amp;= \lfloor x / \sqrt[\downarrow]{u} \rfloor &amp; \text{(cluster number)} \\
    \text{low}(x)        &amp;= x \text{ mod } \sqrt[\downarrow]{u} &amp; \text{(position within cluster)} \\
    \text{index}(x, y)   &amp;= x \sqrt[\downarrow]{u} + y &amp; \text{(reconstructs number)}\end{aligned}$$</span><br /> We have <span class="math inline"><em>x</em> = index(high(<em>x</em>),low(<em>x</em>))</span>. A van Emde Boas tree stores:</p>
<dl>
<dt>Universe size:</dt>
<dd><p><span class="math inline"><em>u</em> = 2<sup><em>k</em></sup></span></p>
</dd>
<dt>Summary, if <span class="math inline"><em>u</em> &gt; 2</span>:</dt>
<dd><p>A van Emde Boas tree of size <span class="math inline">$\sqrt[\uparrow]{u}$</span> summarising the contents of the clusters.</p>
</dd>
<dt>Min and Max:</dt>
<dd><p>The minimum and maximum values of the tree. The minimum value is not included in the clusters.</p>
</dd>
<dt>Cluster array:</dt>
<dd><p><span class="math inline">$\sqrt[\uparrow]{u}$</span> many van Emde Boas tree of size <span class="math inline">$\sqrt[\downarrow]{u}$</span>.</p>
</dd>
</dl>
<p>vEB-Empty-Tree-Insert(V, x) swap(x, V.min) vEB-Tree-Insert(V.summary, high(x)) vEB-Empty-Tree-Insert(V.cluster[high(x)], low(x)) vEB-Tree-Insert(V.cluster[high(x)], low(x)) V.max = x</p>
<p>Characterized by: <br /><span class="math display">$$\begin{aligned}
    T(u)   &amp;\leq T(\sqrt[\uparrow]{u}) + O(1) \\
    T(2^m) &amp;\leq T(2^{\lceil m / 2 \rceil}) + O(1) \\
           &amp;\leq T(2^{2m / 3}) + O(1) \\
    S(m)   &amp;\leq S(2m/3) + O(1) \\
    T(u)   &amp;= T(2^m) = S(m) = O(\lg m) = O(\lg \lg u)\end{aligned}$$</span><br /></p>
<h1 id="np-completeness">NP-completeness</h1>
<p>To define NP-completeness, I first need to define a couple of other things first.</p>
<h2 id="languages">Languages</h2>
<p>A <strong>language</strong> <span class="math inline"><em>L</em></span> over an <strong>alphabet</strong> <span class="math inline"><em>Σ</em></span> is a set of strings made up of symbols from <span class="math inline"><em>Σ</em></span>.</p>
<p>A decision problem <span class="math inline"><em>Q</em></span> defines a language <span class="math inline"><em>L</em></span> over the alphabet <span class="math inline"><em>Σ</em> = {0,1}</span>, in that it contains all the strings <span class="math inline"><em>x</em></span> where <span class="math inline"><em>Q</em>(<em>x</em>)=1</span>: <br /><span class="math display"><em>L</em> = {<em>x</em>∈<em>Σ</em><sup>*</sup>:<em>Q</em>(<em>x</em>)=1}</span><br /> We say that a language <span class="math inline"><em>L</em></span> is <strong>accepted</strong> by an algorithm <span class="math inline"><em>A</em></span> if <span class="math inline"><em>A</em>(<em>x</em>)=1</span> for all <span class="math inline"><em>x</em> ∈ <em>L</em></span>. The algorithm <span class="math inline"><em>A</em></span> <strong>rejects</strong> the language if <span class="math inline"><em>A</em>(<em>x</em>)=0</span>.</p>
<p>A language <span class="math inline"><em>L</em></span> is <strong>decided</strong> by an algorithm <span class="math inline"><em>A</em></span> if for all <span class="math inline"><em>x</em> ∈ <em>L</em></span>, <span class="math inline"><em>A</em>(<em>x</em>)=1</span> and for all <span class="math inline"><em>y</em> ∉ <em>L</em></span>, we have <span class="math inline"><em>A</em>(<em>x</em>)=0</span> (i.e. does not loop forever).</p>
<h2 id="p-and-np">P and NP</h2>
<p>Now define the complexity class <span class="math inline"><em>P</em></span>: <br /><span class="math display">$$\text{P} = {\left\{ L \subseteq {\left\{ 0, 1 \right\}}^* : \text{there exists an algorithm $A$ that decides $L$ in polynomial time.} \right\}}$$</span><br /> Now define NP as the class of languages that can be <em>verified</em> in polynomial time. Then a language belongs to NP when it upholds: <br /><span class="math display">$$L = {\left\{ x \in {\left\{ 0, 1 \right\}}^* : \exists y \text{ where $|y| = O{{\left( |x|^c \right)}}$ such that $A{{\left( x, y \right)}} = 1$} \right\}}$$</span><br /></p>
<h2 id="reducibility">Reducibility</h2>
<p>A language <span class="math inline"><em>L</em><sub>1</sub></span> is polynomial-time reducible to a language <span class="math inline"><em>L</em><sub>2</sub></span>, written <span class="math inline"><em>L</em><sub>1</sub>≤<sub>p</sub><em>L</em><sub>2</sub></span> if there exists a polynomial-time computable function <span class="math inline"><em>f</em> : {0,1}<sup>*</sup> → {0,1}<sup>*</sup></span> such that: <br /><span class="math display">∀<em>x</em> ∈ {0,1}<sup>*</sup> <em>x</em> ∈ <em>L</em><sub>1</sub> ⇔ <em>f</em>(<em>x</em>)∈<em>L</em><sub>2</sub></span><br /></p>
<p><img src="reduction.png" alt="image" /></p>
<h2 id="np-completeness-1">NP-completeness</h2>
<p>We can now define the NP-complete class:</p>
<ol>
<li><p><span class="math inline"><em>L</em> ∈ NP</span>, and</p></li>
<li><p><span class="math inline">∀<em>L</em>′∈NP <em>L</em>′≤<sub>P</sub><em>L</em></span></p></li>
</ol>
<p>Method for showing NP-completeness:</p>
<ol>
<li><p>Prove <span class="math inline"><em>L</em>∈</span> NP</p></li>
<li><p>Select known NP-complete language <span class="math inline"><em>L</em>′</span></p></li>
<li><p>Describe <span class="math inline"><em>f</em></span> which maps <span class="math inline"><em>L</em>′</span> to <span class="math inline"><em>L</em></span></p></li>
<li><p>Prove that <span class="math inline"><em>x</em> ∈ <em>L</em>′⇔<em>f</em>(<em>x</em>)∈<em>L</em></span></p></li>
<li><p>Prove that <span class="math inline"><em>f</em></span> is polynomial-time computable</p></li>
</ol>
<p>NP-complete proofs:</p>
<dl>
<dt><span class="math inline"><em>L</em>≤<sub>P</sub>CIRCUIT-SAT</span>:</dt>
<dd><p>By simulating abstract machine as logic circuit.</p>
</dd>
<dt><span class="math inline">CIRCUIT-SAT≤<sub>P</sub>SAT</span>:</dt>
<dd><p>By assigning a variable to every wire, then constructing a logic formula with the output wire and’ed with ((the other wires) if and only if (the wires input)). For example, <span class="math inline"><em>x</em><sub>1</sub> ∨ <em>x</em><sub>2</sub> = <em>x</em><sub>3</sub></span> gives <span class="math inline"><em>x</em><sub>3</sub> ∧ (<em>x</em><sub>3</sub> ⇔ (<em>x</em><sub>1</sub> ∨ <em>x</em><sub>2</sub>))</span>.</p>
</dd>
<dt><span class="math inline">SAT≤<sub>P</sub>3-CNF-SAT</span>:</dt>
<dd><p>Construct parse tree of the SAT formula, then assign variables to each output. Construct same kind of formula as in SAT. Then look at truth table for each clause and construct 3-CNF formula for each clause by negating the 3-DNF formula.</p>
</dd>
<dt><span class="math inline">3-CNF-SAT≤<sub>P</sub>CLIQUE</span>:</dt>
<dd><p>Construct graph with a node for each literal in every clause. Connect nodes that are (1) not in the same clause and (2) are not the negation of each other.</p>
<p>If <span class="math inline"><em>ϕ</em></span> is satisfiable, then pick the literals in the clauses that are 1, then those form a clique of size <span class="math inline"><em>k</em></span>. There must be edges between them because they can’t be complements since they are all 1.</p>
<p>If <span class="math inline"><em>G</em></span> has a clique of size <span class="math inline"><em>k</em></span> then that clique must have a vertex in every triple, since there are no edges between vertices in the same triple. Then assign 1 to each such vertex. We know we won’t be assigning 1 to both a literal and its complement since no edges are between them. Set variables not in the clique arbitrarily.</p>
</dd>
<dt><span class="math inline">CLIQUE≤<sub>P</sub>VERTEX-COVER</span>:</dt>
<dd><p>Consider a graph <span class="math inline"><em>G</em></span> with a clique <span class="math inline"><em>V</em>′</span> of size <span class="math inline"><em>k</em></span>. Construct complement, <span class="math inline">$\bar{G}$</span>. Then vertex cover is <span class="math inline"><em>V</em> − <em>V</em>′</span>.</p>
<p>Take any edge in <span class="math inline">$\bar{G}$</span>. One of the nodes must be in <span class="math inline"><em>V</em> − <em>V</em>′</span>, since they both can’t be in <span class="math inline"><em>V</em>′</span>, cause then the edge would not exist. So that edge is covered by <span class="math inline"><em>V</em> − <em>V</em>′</span> which has size <span class="math inline">|<em>V</em>|−<em>k</em></span> and same can be said for every other edge, therefore it is a vertex cover.</p>
<p>Take any two vertices not in vertex cover <span class="math inline"><em>V</em>′</span> of <span class="math inline">$\bar{G}$</span> with size <span class="math inline">|<em>V</em>|−<em>k</em></span>. Then the vertices must share an edge in <span class="math inline"><em>G</em></span> since otherwise <span class="math inline"><em>V</em>′</span> would not be a vertex cover. Hence <span class="math inline"><em>V</em> − <em>V</em>′</span> is a clique in <span class="math inline"><em>G</em></span> since they must all have edges between them. Size is <span class="math inline">|<em>V</em>|−|<em>V</em>′| = |<em>V</em>|−(|<em>V</em>|−<em>k</em>)=<em>k</em></span>.</p>
</dd>
<dt><span class="math inline">VERTEX-COVER≤<sub>P</sub>HAM-CYCLE</span>:</dt>
<dd><p>Not curriculum.</p>
</dd>
<dt><span class="math inline">HAM-CYCLE≤<sub>P</sub>TSP</span>:</dt>
<dd><p>Consider graph <span class="math inline"><em>G</em></span> with hamiltonian cycle <span class="math inline"><em>h</em></span>. Then construct complete TSP graph <span class="math inline"><em>G</em>′</span> where cost of an edge is 0 if it was in the old graph and 1 if it wasn’t in the old graph.</p>
<p>Then if <span class="math inline"><em>G</em></span> has a hamiltonian cycle, then <span class="math inline"><em>G</em>′</span> must have a TSP tour of cost 0, since it’s just the edges of <span class="math inline"><em>h</em></span>.</p>
<p>If <span class="math inline"><em>G</em>′</span> has a tour of cost 0, then each edge on the tour must have cost 0, which means that all those edges are in <span class="math inline"><em>G</em></span>. Therefore there must also be a hamiltonian cycle in <span class="math inline"><em>G</em></span>.</p>
</dd>
<dt><span class="math inline">3-CNF-SAT≤<sub>P</sub>SUBSET-SUM</span>:</dt>
<dd><p>Draw huge table with variables and clauses as columns and literals and slack variables as rows. Then blah blah. Fuck me if they ask about this one.</p>
</dd>
</dl>
<h1 id="exact-exponential-algorithms-and-parameterized-complexity">Exact exponential algorithms and<br />
parameterized complexity</h1>
<h2 id="motivation-3">Motivation</h2>
<p>There are many very useful problems that we know to be NP-complete and thus we cannot hope to solve them in polynomial time. However, sometimes we just have to solve these difficult problems anyway and thus we’d like algorithms that make the best of the running time constraints we have.</p>
<h2 id="exact-exponential-algorithms-dynamic-travelling-salesman">Exact exponential algorithms: Dynamic Travelling Salesman</h2>
<p>Given some cities <span class="math inline"><em>c</em><sub>1</sub>, …, <em>c</em><sub><em>n</em></sub></span> and distances between each one, find minimal permutation such that: <br /><span class="math display">$$\sum_{i = 1}^{n - 1} {\left[ d {\left( c_{\pi (i)}, c_{\pi (i + 1)} \right)} \right]} + d {\left( c_{\pi (n)}, c_{\pi (1)} \right)}$$</span><br /> <strong>Naive Approach:</strong> Try all permutations. There are <span class="math inline"><em>n</em>!</span>, so <span class="math inline"><em>O</em>(<em>n</em>!)</span>.</p>
<p><strong>Better idea:</strong> Use dynamic programming. <br /><span class="math display"><em>S</em> ⊆ {<em>c</em><sub>2</sub>,…,<em>c</em><sub><em>n</em></sub>}</span><br /> <br /><span class="math display">$$OPT {\left[ S, c_i \right]} = \text{Minimum length of tour starting in $c_1$, visiting all in $S$, ending in $c_i$}$$</span><br /> Compute <span class="math inline"><em>O</em><em>P</em><em>T</em>[<em>S</em>,<em>c</em><sub><em>i</em></sub>]</span> in increasing cardinality of <span class="math inline"><em>S</em></span> (i.e:<br />
<span class="math inline"><em>O</em><em>P</em><em>T</em>[{<em>c</em><sub>2</sub>},<em>c</em><sub>2</sub>]</span>, <span class="math inline"><em>O</em><em>P</em><em>T</em>[{<em>c</em><sub>3</sub>},<em>c</em><sub>3</sub>]</span>, <span class="math inline">…</span>, <span class="math inline"><em>O</em><em>P</em><em>T</em>[{<em>c</em><sub><em>n</em></sub>},<em>c</em><sub><em>n</em></sub>]</span>, <span class="math inline"><em>O</em><em>P</em><em>T</em>[{<em>c</em><sub>2</sub>,<em>c</em><sub>3</sub>},<em>c</em><sub>2</sub>]</span>,)</p>
<p>Trivial if <span class="math inline">|<em>S</em>|=1</span>: Simply <span class="math inline"><em>O</em><em>P</em><em>T</em>[<em>S</em>,<em>c</em><sub><em>i</em></sub>] = <em>d</em>(<em>c</em><sub>1</sub>, <em>c</em><sub><em>i</em></sub>)</span>.</p>
<p>Use the previous tours to find next tour, due to (assuming <span class="math inline"><em>c</em><sub><em>j</em></sub></span> is right before <span class="math inline"><em>c</em><sub><em>i</em></sub></span>): <br /><span class="math display"><em>O</em><em>P</em><em>T</em>[<em>S</em>,<em>c</em><sub><em>i</em></sub>] = <em>O</em><em>P</em><em>T</em>[<em>S</em>\<em>c</em><sub><em>i</em></sub>,<em>c</em><sub><em>j</em></sub>] + <em>d</em>(<em>c</em><sub><em>j</em></sub>,<em>c</em><sub><em>i</em></sub>)</span><br /> This leads to: <br /><span class="math display"><em>O</em><em>P</em><em>T</em>[<em>S</em>,<em>c</em><sub><em>i</em></sub>] = min{<em>O</em><em>P</em><em>T</em>[<em>S</em>\{<em>c</em><sub><em>i</em></sub>},<em>c</em><sub><em>j</em></sub>]+<em>d</em>(<em>c</em><sub><em>j</em></sub>,<em>c</em><sub><em>i</em></sub>):<em>c</em><sub><em>j</em></sub>∈<em>S</em>\{<em>c</em><sub><em>i</em></sub>}}</span><br /> Lastly, we want: <br /><span class="math display">min{<em>O</em><em>P</em><em>T</em>[{<em>c</em><sub>2</sub>,<em>c</em><sub>3</sub>,…,<em>c</em><sub><em>n</em></sub>},<em>c</em><sub><em>i</em></sub>]+<em>d</em>(<em>c</em><sub><em>i</em></sub>,<em>c</em><sub>1</sub>):<em>i</em>∈{2,3,…,<em>n</em>}}</span><br /></p>
<p><span class="math inline"><em>O</em><em>P</em><em>T</em>{<em>c</em><sub><em>i</em></sub>,<em>c</em><sub><em>i</em></sub>} = <em>d</em>(<em>c</em><sub>1</sub>,<em>c</em><sub><em>i</em></sub>)</span> <span class="math inline"><em>O</em><em>P</em><em>T</em>[<em>S</em>,<em>c</em><sub><em>i</em></sub>] = min{<em>O</em><em>P</em><em>T</em>[<em>S</em>\{<em>c</em><sub><em>i</em></sub>},<em>c</em><sub><em>k</em></sub>]+<em>d</em>(<em>c</em><sub><em>k</em></sub>,<em>c</em><sub><em>i</em></sub>):<em>c</em><sub><em>k</em></sub>∈<em>S</em>\{<em>c</em><sub><em>i</em></sub>}}</span> <span class="math inline">min{<em>O</em><em>P</em><em>T</em>[{<em>c</em><sub>2</sub>,<em>c</em><sub>3</sub>,…,<em>c</em><sub><em>n</em></sub>},<em>c</em><sub><em>i</em></sub>]+<em>d</em>(<em>c</em><sub><em>i</em></sub>,<em>c</em><sub>1</sub>):<em>i</em>∈{2,3,…,<em>n</em>}}</span></p>
<h2 id="paremeterized-algorithms-vertex-cover-aka-bar-fight-prevention">Paremeterized algorithms: Vertex Cover<br />
aka “Bar Fight Prevention”</h2>
<p>For certain problems, we might know some constraints on the kind of solution we want. This is called parameterization. We give the algorithm some extra information and in return we can design the algorithm in a clever way such that the exponential parts of it only depend on the parameters we want it to depend on.</p>
<h3 id="parameterize-size-k-of-vertex-cover">Parameterize size <span class="math inline"><em>k</em></span> of vertex cover</h3>
<p>By parameterizing the size of the vertex cover solution, we can get a potentially much more efficient algorithm for it:</p>
<p><strong>Core idea:</strong> Iterate through edges <span class="math inline">(<em>u</em>, <em>v</em>)</span>, deciding whether <span class="math inline"><em>u</em></span> or <span class="math inline"><em>v</em></span> should be in the cover or not. Call recursively two times, one where <span class="math inline"><em>u</em></span> is picked and one where <span class="math inline"><em>v</em></span> is picked.</p>
<p>Leads to decision tree of depth <span class="math inline"><em>k</em></span>, therefore making <span class="math inline"><em>O</em>(2<sup><em>k</em></sup>)</span> recursive calls, each taking <span class="math inline"><em>O</em>(<em>m</em>)</span> time. <span class="math inline"><em>m</em></span> is bounded by <span class="math inline"><em>n</em><em>k</em>/2</span> because we can easily include any vertices in the cover that has more than <span class="math inline"><em>k</em></span> edges. So the running time is <span class="math inline"><em>O</em>(<em>n</em><em>k</em>2<sup><em>k</em></sup>)</span>.</p>
<p><img src="bargraph.png" alt="image" /></p>
<p><img src="bartree.png" alt="image" /></p>
<p>This is an example of a <em>fixed-parameter algorithm</em> because it has the form <span class="math inline"><em>f</em>(<em>k</em>)⋅<em>n</em><sup><em>c</em></sup></span>, i.e. the algorithm is constantly polynomial in <span class="math inline"><em>n</em></span> but may be exponential or worse in <span class="math inline"><em>k</em></span>.</p>
<h1 id="approximation-algorithms">Approximation algorithms</h1>
<h2 id="motivation-4">Motivation</h2>
<p>Sometimes we have to solve NP-complete problems, but unfortunately this can take a very long time. In these cases, we must either wait for the computation or we must be satisfied with an approximate solution which can be calculated fast. This is what we use approximation algorithms for.</p>
<p>We talk about approximation ratios: <br /><span class="math display">$$\max{\left(  \frac{C^*}{C}, \frac{C}{C^*}  \right)} \leq \rho(n)$$</span><br /></p>
<h2 id="approximate-travelling-salesman-problem-with-triangle-inequality">Approximate travelling salesman problem with triangle inequality</h2>
<p>Algorithm goes like this:</p>
<ol>
<li><p>Select a vertex to be the root</p></li>
<li><p>Compute minimum spanning tree from root by Prim’s algorithm</p></li>
<li><p>Get a vertex ordered by first visit in preorder tree walk of the tree</p></li>
<li><p>Return that order</p></li>
</ol>
<p>Algorithm is quite obviously polynomial.</p>
<p><img src="tspExample.png" alt="image" /></p>
<p>The cost of a minimum spanning tree must be lower or equal to the cost of the tree gotten by removing an edge from the optimal tour. <br /><span class="math display"><em>c</em>(<em>T</em>)≤<em>c</em>(<em>H</em><sup>*</sup>)</span><br /> A full walk is double the cost of the tree since it goes through every edge twice <br /><span class="math display"><em>c</em>(<em>W</em>)=2<em>c</em>(<em>T</em>)</span><br /> Which means: <br /><span class="math display"><em>c</em>(<em>W</em>)=2<em>c</em>(<em>T</em>)≤2<em>c</em>(<em>H</em><sup>*</sup>)</span><br /> The algorithm works by simply removing the unnecessary double visits to the nodes and thus making the walk shorter, resulting in a tour <span class="math inline"><em>H</em></span>. This only works because of the triangle inequality. Then we have: <br /><span class="math display"><em>c</em>(<em>H</em>)≤<em>c</em>(<em>W</em>)=2<em>c</em>(<em>T</em>)≤2<em>c</em>(<em>H</em><sup>*</sup>)</span><br /> Hence it is a 2-approximation algorithm.</p>
<h2 id="approximate-max-3-cnf">Approximate MAX-3-CNF</h2>
<p>Now we talk about <em>expected cost</em>. Want to find the maximum number of clauses we can satisfy in a 3-CNF formula with <span class="math inline"><em>m</em></span> clauses.</p>
<p>Algorithm simply sets each variable by a coin flip. Let <span class="math inline"><em>Y</em><sub><em>i</em></sub></span> be the indicator variable of the <span class="math inline"><em>i</em></span>th clause being satisfied.</p>
<p>Chance of a clause not being satisfied is the same as all 3 variables set to 0, i.e: <span class="math inline">(1/2)<sup>3</sup></span>. Then, chance of a clauses being satisfied is: <br /><span class="math display">Pr[<em>Y</em><sub><em>i</em></sub>=1] = 1 − (1/2)<sup>3</sup> = 7/8</span><br /> Expected number of satisfied clauses is then: <br /><span class="math display">$$\begin{aligned}
    \mathbb{E}{\left[ Y \right]} &amp;= \mathbb{E}{\left[ \sum_{i = 1}^m Y_i \right]} \\
                          &amp;= \sum_{i = 1}^m \mathbb{E}{\left[ Y_i \right]} \\
                          &amp;= \sum_{i = 1}^m 7/8 \\
                          &amp;= 7m/8\end{aligned}$$</span><br /> You can have at most <span class="math inline"><em>m</em></span> satisfied clauses, hence approximation ratio is at most: <br /><span class="math display">$$\frac{m}{7m/8} = 8/7$$</span><br /></p>
<h2 id="other-approximations">Other approximations</h2>
<dl>
<dt>Vertex cover:</dt>
<dd><p>Take random edge every time and add both vertices to the cover, removing their edges. Can at most be twice as bad since every edge must be covered and you take two vertices per edge.</p>
</dd>
<dt>Set-cover:</dt>
<dd><p>Greedily pick the set with most elements. Remove elements and continue. Gives <span class="math inline"><em>ρ</em>(<em>n</em>) = <em>H</em>(max{|<em>S</em>|:<em>S</em>∈ℱ})</span>.</p>
</dd>
<dt>Weighted vertex cover by linear programming:</dt>
<dd><p>Construct integer linear program where you minimize sum of weights times values. Every edge <span class="math inline">(<em>u</em>, <em>v</em>)</span> must have the sum of their vertices be at least 1. Now relax the problem (allow floating numbers), then take the vertices with more than 1/2 value. Then it is a 2-approximation algorithm.</p>
</dd>
<dt>Subset-sum:</dt>
<dd><p>Create list of sums, but trim within a percentage. Fully polynomial approximation scheme. This means that it runs polynomially in both <span class="math inline">1/<em>ϵ</em></span> and <span class="math inline"><em>n</em></span>.</p>
</dd>
</dl>
<p><img src="text.png" alt="image" /></p>
</body>
</html>
